{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19837d0b",
   "metadata": {},
   "source": [
    "# 张量的创建和常用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4deff46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.11.0+cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a4c00d",
   "metadata": {},
   "source": [
    "## 张量（Tensor）的基本创建及其类型\n",
    "\n",
    "&emsp;&emsp;和NumPy中的dnarray一样，张量的本质也是结构化的组织了大量的数据。并且，在实际操作过程中，张量的创建和基本功能也和NumPy中的array非常类似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b387f75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过列表创建张量\n",
    "t = torch.tensor([1, 2])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368092be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过元组创建张量\n",
    "torch.tensor((1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array((1, 2))\n",
    "t1 = torch.tensor(a)\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6a5e1f",
   "metadata": {},
   "source": [
    "### 2.张量的类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9958d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数组类型\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89701848",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31b6017",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在这里，我们发现，整数型的数组默认创建int32（整型）类型，而张量则默认创建int64（长整型）类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1b250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor(np.array([1.1, 2.2])).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6a2b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.tensor([True, False])\n",
    "t2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb88c94",
   "metadata": {},
   "source": [
    "**<center>PyTorch中Tensor类型</center>**\n",
    "\n",
    "|数据类型|dtype|      \n",
    "|:--:|:--:|      \n",
    "|32bit浮点数|torch.float32或torch.float|      \n",
    "|64bit浮点数|torch.float64或torch.double|\t      \n",
    "|16bit浮点数|torch.float16或torch.half|\t      \n",
    "|8bit无符号整数|torch.unit8|    \n",
    "|8bit有符号整数|torch.int8|\n",
    "|16bit有符号整数|torch.int16或torch.short|\n",
    "|16bit有符号整数|torch.int16或torch.short|\n",
    "|32bit有符号整数|torch.int32或torch.int|\n",
    "|64bit有符号整数|torch.int64或torch.long|\n",
    "|布尔型|torch.bool|\n",
    "|复数型|torch.complex64|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ee6b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当然，在PyTorch中也支持复数类型对象创建\n",
    "a = torch.tensor(1 + 2j)           # 1是实部、2是虚部\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a0c5f3",
   "metadata": {},
   "source": [
    "### 3.张量类型的转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 浮点型和整数型的隐式转化\n",
    "torch.tensor([1.1, 2]).dtype "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9895d1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转化为16位整数\n",
    "t.short() # long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876fd9a4",
   "metadata": {},
   "source": [
    "## 二、张量的维度与形变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebd2d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1, 2])\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959efc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.ndim\n",
    "t1.shape\n",
    "t1.size()\n",
    "t1.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23950af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用list的list创建二维数组\n",
    "t2 = torch.tensor([[1, 2], [3, 4]])\n",
    "t2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6837cca4",
   "metadata": {},
   "source": [
    "- 高维张量\n",
    "\n",
    "&emsp;&emsp;一般来说，三维及三维以上的张量，我们就将其称为高维张量。当然，在高维张量中，最常见的还是三维张量。我们可以将其理解为二维数组或者矩阵的集合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3ecdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.array([[1, 2, 2], [3, 4, 4]])\n",
    "a2 = np.array([[5, 6, 6], [7, 8, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259afc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由两个形状相同的二维数组创建一个三维的张量\n",
    "t3 = torch.tensor([a1, a2])\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cad479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3.flatten() # 展平"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b5815a",
   "metadata": {},
   "source": [
    "#### 2.2 reshape方法：任意变形"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d9ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.reshape(1, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6682c478",
   "metadata": {},
   "source": [
    "## 二、特殊张量的创建方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee079fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全0张量\n",
    "torch.zeros([2, 3])            # 创建全是0的，两行、三列的张量（矩阵）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e7c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.ones([2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd73e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.eye(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3fe0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.diag([1, 2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71093b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(2, 3)\n",
    "torch.normal(2, 3, size = (2, 2))            # 均值为2，标准差为3的张量\n",
    "torch.randint(1, 10, [2, 4])                 # 在1-10之间随机抽取整数，组成两行四列的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f16b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(5)                              # 和range相同\n",
    "torch.arange(1, 5, 0.5)                      # 从1到5（左闭右开），每隔0.5取值一个\n",
    "torch.linspace(1, 5, 3)                      # 从1到5（左右都包含），等距取三个数\n",
    "torch.empty(2, 3)\n",
    "torch.full([2, 4], 2)                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a60f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.full_like(t1, 2)             # 根据t1形状，填充数值2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e55c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randint_like(t2, 1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e9cf88",
   "metadata": {},
   "source": [
    "## 三、张量（Tensor）和其他相关类型之间的转化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d19938",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.numpy() # 转成数组\n",
    "np.array(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e65d4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef15dc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(t1) # 将为每一个转成tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87adebf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.item() # 获得对应数值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa07468",
   "metadata": {},
   "source": [
    "## 四、张量的深拷贝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de47fa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ecb24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t11 = t1                          # t11是t1的浅拷贝       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6dd588",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1[1]                             # 此时t11也会发生修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db39a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "t11 = t1.clone() # 这样可以存在不同空间，此时就不会发生相对应的变化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18ec3b4",
   "metadata": {},
   "source": [
    "# Lesson 2.张量的索引、分片、合并以及维度调整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a35dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9bafbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.arange(1, 11)\n",
    "t1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb10895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1[1: 8]             # 索引其中2-9号元素，并且左包含右不包含\n",
    "t1[1: 8: 2]          # 索引其中2-9号元素，左包含右不包含，且隔两个数取一个"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ded88fc",
   "metadata": {},
   "source": [
    "### 2.二维张量索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2ebd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.arange(1, 10).reshape(3, 3)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2071aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2[0, 1]                  # 表示索引第一行、第二个（第二列的）元素\n",
    "t2[0, ::2]                # 表示索引第一行、每隔两个元素取一个\n",
    "t2[0, [0, 2]]             # 索引结果同上\n",
    "t2[::2, ::2]              # 表示每隔两行取一行、并且每一行中每隔两个元素取一个\n",
    "t2[[0, 2], 1]              # 索引第一行、第三行、第二列的元素"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3834b358",
   "metadata": {},
   "source": [
    "### 3.三维张量的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7e02d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = torch.arange(1, 28).reshape(3, 3, 3)\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedeeede",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3[1, 1, 1]          # 索引第二个矩阵中，第二行、第二个元素\n",
    "t3[1, ::2, ::2]      # 索引第二个矩阵，行和列都是每隔两个取一个"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a372b347",
   "metadata": {},
   "source": [
    "## 二、张量的函数索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fdb8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.arange(1, 11)\n",
    "indices = torch.tensor([1, 2])\n",
    "torch.index_select(t1, 0, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad066cc",
   "metadata": {},
   "source": [
    "在index_select函数中，第二个参数实际上代表的是索引的维度。对于t1这个一维向量来说，由于只有一个维度，因此第二个参数取值为0，就代表在第一个维度上进行索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd779aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.arange(12).reshape(4, 3)\n",
    "torch.index_select(t2, 0, indices)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b2193",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.index_select(t2, 1, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d53d8f",
   "metadata": {},
   "source": [
    "## 三、tensor.view()方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1335b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(6).reshape(2, 3)\n",
    "te = t.view(3, 2)              # 构建一个数据相同，但形状不同的“视图”\n",
    "te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dc2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = t.view(1, 2, 3)           # 维度也可以修改\n",
    "tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64ed997",
   "metadata": {},
   "source": [
    "## 四、张量的分片函数\n",
    "### 1.分块：chunk函数\n",
    "&emsp;&emsp;chunk函数能够按照某维度，对张量进行均匀切分，并且返回结果是原张量的视图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b52d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.arange(12).reshape(4, 3)\n",
    "tc = torch.chunk(t2, 4, dim=0)           # 在第零个维度上（按行），进行四等分\n",
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22756d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc[0]\n",
    "tc[0][0]\n",
    "tc[0][0][0] =1 # 修改tc对应的t2也会修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bfd62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.chunk(t2, 3, dim=0)            # 次一级均分结果，不能等分返回次一些等分函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d172ec00",
   "metadata": {},
   "source": [
    "### 2.拆分：split函数\n",
    "&emsp;&emsp;split既能进行均分，也能进行自定义切分。当然，需要注意的是，和chunk函数一样，split返回结果也是view。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a88f919",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.arange(12).reshape(4, 3)\n",
    "torch.split(t2, 2, 0)           # 第二个参数只输入一个数值时表示均分，第三个参数表示切分的维度\n",
    "torch.split(t2, [1, 3], 0)           # 第二个参数输入一个序列时，表示按照序列数值进行切分，也就是1/3分\n",
    "torch.split(t2, [1, 1, 2], 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f029664",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = torch.split(t2, [1, 2], 1) \n",
    "ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dd4c66",
   "metadata": {},
   "source": [
    "## 五、张量的合并操作\n",
    "\n",
    "&emsp;&emsp;张量的合并操作类似与列表的追加元素，可以拼接、也可以堆叠。\n",
    "\n",
    "- 拼接函数：cat\n",
    "\n",
    "PyTorch中，可以使用cat函数实现张量的拼接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b0c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(2, 3)\n",
    "b = torch.ones(2, 3)\n",
    "c = torch.zeros(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907587a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([a, b])                  # 按照行进行拼接，dim默认取值为0\n",
    "torch.cat([a, b], 1)               # 按照列进行拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e446e783",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([a, c], 1)               # 形状不匹配时将报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11efa2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([a, b])                 # 堆叠之后，生成一个三维张量\n",
    "torch.stack([a, b]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b335a5",
   "metadata": {},
   "source": [
    "## 六、张量维度变换\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e1c119",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(4)\n",
    "a2 = a.reshape(1, 4)\n",
    "a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f266331",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.squeeze(a2).ndim  # - squeeze函数：删除不必要的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490bd84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.zeros(1, 2, 1, 2)\n",
    "t.shape\n",
    "torch.unsqueeze(t, dim = 0)     # 在第1个维度索引上升高1个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2768dc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unsqueeze(t, dim = 2).shape         # 在第3个维度索引上升高1个维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5cf83",
   "metadata": {},
   "source": [
    "# Lesson 3.张量的广播和科学运算\n",
    "\n",
    " - 数学运算的分类      \n",
    " PyToch总共为Tensor设计了六大类数学运算，分别是：      \n",
    "- 1.逐点运算（Pointwise Ops）：指的是针对Tensor中每个元素执行的相同运算操作；      \n",
    "- 2.规约运算（Reduction Ops）：指的是对于某一张量进行操作得出某种总结值；      \n",
    "- 3.比较运算（Comparison Ops）：指的是对多个张量进行比较运算的相关方法；      \n",
    "- 4.谱运算（Spectral Ops）：指的是涉及信号处理傅里叶变化的操作；        \n",
    "- 5.BLAS和LAPACK运算：指的是基础线性代数程序集（Basic Linear Algeria Subprograms）和线性代数包（Linear Algeria Package）中定义的、主要用于线性代数科学计算的函数和方法；      \n",
    "- 6.其他运算（Other Ops）：其他未被归类的数学运算。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6805a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.arange(3)\n",
    "t1 + t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 不同形状之间的张量进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68f874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 + 1                                     # 1是标量，可以看成是零维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79355a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = torch.zeros((3, 4))\n",
    "t3 = torch.zeros(3, 4, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e603157",
   "metadata": {},
   "outputs": [],
   "source": [
    "t21 = torch.ones(1, 4)\n",
    "t21 + t2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e429a6",
   "metadata": {},
   "source": [
    "<img src=\"https://i.loli.net/2021/01/13/kYhtA9bWgNQsiec.jpg\" alt=\"1\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a28dd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "t22 = torch.ones(3, 1)\n",
    "t22 + t2              # 形状为（3，1）的张量和形状为（3，4）的张量相加，可以广播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adc8b1f",
   "metadata": {},
   "source": [
    "<img src=\"https://i.loli.net/2021/01/13/Y7bd8sNRO4iC5mZ.jpg\" alt=\"2\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bc116c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t24 = torch.arange(3).reshape(3, 1)\n",
    "t25 = torch.arange(3).reshape(1, 3)\n",
    "t24 + t25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05de5e60",
   "metadata": {},
   "source": [
    "![4](https://i.loli.net/2021/01/13/rX53dcTDMBxGmg4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8ce232",
   "metadata": {},
   "source": [
    "## 三维广播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4232692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = torch.zeros(3, 4, 5)\n",
    "t31 = torch.ones(3, 4, 1)\n",
    "t3 + t31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b435d913",
   "metadata": {},
   "outputs": [],
   "source": [
    "t32 = torch.ones(3, 1, 5)\n",
    "t32 + t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567863ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "t33 = torch.ones(1, 1, 5)\n",
    "t3 + t33"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aff4887",
   "metadata": {},
   "source": [
    "<img src=\"https://i.loli.net/2021/01/13/eFR3UnsEwz1v4Ax.jpg\" alt=\"3\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f0fa0c",
   "metadata": {},
   "source": [
    "注：此处标注的两次广播，我们也可认为上述全部过程的实现是一次“大的”广播。同时，此处最开始的t33也就相当于一个一维的、包含五个元素的张量，因此上述过程也可视为一个一维张量和一个三维张量计算时的广播过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d7d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二维张量转化为三维张量\n",
    "t2 = torch.arange(4).reshape(2, 2)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac7c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "t2.reshape(1, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4409958",
   "metadata": {},
   "source": [
    "## 二、逐点运算（Pointwise Ops）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6303f6e",
   "metadata": {},
   "source": [
    "&emsp;&emsp;PyTorch中逐点运算大部分都是可以针对Tensor中每个元素都进行的数学科学运算，并且都是较为通用的数学科学运算，和NumPy中针对Array的科学运算类似。在PyTorch中文文档中有全部运算符的相关介绍，此处仅针对常用计算函数进行介绍。      \n",
    "&emsp;&emsp;逐点运算主要包括数学基本运算、数值调整运算和数据科学运算三块，相关函数如下：\n",
    "\n",
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.add(t1，t2 )      | t1、t2两个张量逐个元素相加，等效于t1+t2     |\n",
    "| torch.subtract(t1，t2) | t1、t2两个张量逐个元素相减，等效于t1-t2      |\n",
    "| torch.multiply(t1，t2) | t1、t2两个张量逐个元素相乘，等效于t1\\*t2            |\n",
    "| torch.divide(t1，t2)   | t1、t2两个张量逐个元素相除，等效于t1/t2            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a53c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1, 2])\n",
    "t2 = torch.tensor([3, 4])\n",
    "t1 + t2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c17f023",
   "metadata": {},
   "source": [
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.abs(t)        | 返回绝对值 | \n",
    "| torch.ceil(t)       | 向上取整 | \n",
    "| torch.floor(t)      | 向下取整 |\n",
    "| torch.round(t)      | 四舍五入取整 |\n",
    "| torch.neg(t)      | 返回相反的数 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b631b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randn(5)\n",
    "torch.round(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0aeedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.abs(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac45aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.neg(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbe69e7",
   "metadata": {},
   "source": [
    "而若要对原对象本身进行修改，则可考虑使用`方法_()`的表达形式，对对象本身进行修改。此时方法就是上述同名函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926ef663",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.abs_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fffd8e2",
   "metadata": {},
   "source": [
    "|**数学运算函数**|**数学公式**|**描述**|\n",
    "| :------:| :------: | :------: |\n",
    "| 幂运算 |\n",
    "| torch.exp(t)        |$ y_{i} = e^{x_{i}} $ | 返回以e为底、t中元素为幂的张量 | \n",
    "| torch.expm1(t)         | $ y_{i} = e^{x_{i}} $ - 1 |对张量中的所有元素计算exp（x） - 1|\n",
    "| torch.exp2(t)          | $ y_{i} = 2^{x_{i}} $ |逐个元素计算2的t次方。 | \n",
    "| torch.pow(t,n)       | $\\text{out}_i = x_i ^ \\text{exponent} $ | 返回t的n次幂 | \n",
    "| torch.sqrt(t)       |$ \\text{out}_{i} = \\sqrt{\\text{input}_{i}} $ | 返回t的平方根 | \n",
    "| torch.square(t)        |$ \\text{out}_i = x_i ^ \\text{2} $ | 返回输入的元素平方。                     | \n",
    "| 对数运算 |\n",
    "| torch.log10(t)      |$ y_{i} = \\log_{10} (x_{i}) $ | 返回以10为底的t的对数 | \n",
    "| torch.log(t)  |$ y_{i} = \\log_{e} (x_{i}) $| 返回以e为底的t的对数 |\n",
    "| torch.log2(t)          |$ y_{i} = \\log_{2} (x_{i}) $| 返回以2为底的t的对数                         | \n",
    "| torch.log1p(t)         |$ y_i = \\log_{e} (x_i $ + 1)| 返回一个加自然对数的输入数组。     | \n",
    "| 三角函数运算|\n",
    "| torch.sin(t)           |三角正弦。                               | \n",
    "| torch.cos(t)           | 元素余弦。                               | \n",
    "| torch.tan(t)           |逐元素计算切线。                         | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a2f26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.pow(torch.tensor(2), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171f8875",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(1, 4)\n",
    "t1 = t.float()\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21645c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.expm1(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8cee40",
   "metadata": {},
   "source": [
    "注，此处返回结果是$e^{t} - 1$，在数值科学计算中，expm1函数和log1p函数是一对对应的函数关系，后面再介绍log1p的时候会讲解这对函数的实际作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a33c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 降序排列\n",
    "torch.sort(t, descending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5531547c",
   "metadata": {},
   "source": [
    "## 三、规约运算(均值/方差)\n",
    "\n",
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.mean(t)        | 返回张量均值 | \n",
    "| torch.var(t)       | 返回张量方差 | \n",
    "| torch.std(t)        | 返回张量标准差 | \n",
    "| torch.var_mean(t)       | 返回张量方差和均值 | \n",
    "| torch.std_mean(t)       | 返回张量标准差和均值 | \n",
    "| torch.max(t)        | 返回张量最大值 | \n",
    "| torch.argmax(t)        | 返回张量最大值索引 | \n",
    "| torch.min(t)       | 返回张量最小值 | \n",
    "| torch.argmin(t)       | 返回张量最小值索引 | \n",
    "| torch.median(t)        | 返回张量中位数 | \n",
    "| torch.sum(t)       | 返回张量求和结果 | \n",
    "| torch.logsumexp(t)       | 返回张量各元素求和结果，适用于数据量较小的情况 | \n",
    "| torch.prod(t)        | 返回张量累乘结果 | \n",
    "| torch.dist(t1, t2)        | 计算两个张量的闵式距离，可使用不同范式 |\n",
    "| torch.topk(t)        | 返回t中最大的k个值对应的指标 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f980d1b",
   "metadata": {},
   "source": [
    "&emsp;&emsp;dist函数可计算闵式距离（闵可夫斯基距离），通过输入不同的p值，可以计算多种类型的距离，如欧式距离、街道距离等。闵可夫斯基距离公式如下：      \n",
    "<center> $ D(x,y) = (\\sum^{n}_{u=1}|x_u-y_u|^{p})^{1/p}$ </center>\n",
    "\n",
    "p取值为2时，计算欧式距离"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5267ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.dist(t1, t2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dfdd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个2*3*4的三维张量\n",
    "t3 = torch.arange(24).float().reshape(2, 3, 4)\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d480285",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(t3, dim = 0)\n",
    "torch.sum(t3, dim = 1)\n",
    "torch.sum(t3, dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a78d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 默认情况下，是按照行进行升序排序，排序，降序\n",
    "torch.sort(t22, dim = 1, descending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0c830e",
   "metadata": {},
   "source": [
    "### 四、比较运算\n",
    "\n",
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.eq(t1, t2)        | 比较t1、t2各元素是否相等，等效==| \n",
    "| torch.equal(t1, t2)       | 判断两个张量是否是相同的张量 | \n",
    "| torch.gt(t1, t2)        | 比较t1各元素是否大于t2各元素，等效>| \n",
    "| torch.lt(t1, t2)        | 比较t1各元素是否小于t2各元素，等效<| \n",
    "| torch.ge(t1, t2)        | 比较t1各元素是否大于或等于t2各元素，等效>=| \n",
    "| torch.le(t1, t2)        | 比较t1各元素是否小于等于t2各元素，等效<=| \n",
    "| torch.ne(t1, t2)        | 比较t1、t2各元素是否不相同，等效!=| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5fe8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([1.0, 3, 4])\n",
    "t2 = torch.tensor([1.0, 2, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9942a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 == t2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a47bde",
   "metadata": {},
   "source": [
    "# Lesson 4.张量的线性代数运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e841ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(5)\n",
    "# 对角线向上偏移一位，-1相反\n",
    "torch.diag(t, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e81a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.arange(9).reshape(3, 3)\n",
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa01e37",
   "metadata": {},
   "source": [
    "## 上三角和下三角"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12518805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取上三角矩阵\n",
    "torch.triu(t1)\n",
    "# 上三角矩阵向左下偏移一位，另一方向偏移同理\n",
    "torch.triu(t1, -1)\n",
    "# 下三角矩阵\n",
    "torch.tril(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acf2f97",
   "metadata": {},
   "source": [
    "### 矩阵的基本计算\n",
    "\n",
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.dot(t1, t2)        | 计算t1、t2张量内积 | \n",
    "| torch.mm(t1, t2)        | 矩阵乘法 | \n",
    "| torch.mv(t1, t2)        | 矩阵乘向量 | \n",
    "| torch.bmm(t1, t2)        | 批量矩阵乘法 | \n",
    "| torch.addmm(t, t1, t2)        | 矩阵相乘后相加 | \n",
    "| torch.addbmm(t, t1, t2)        | 批量矩阵相乘后相加 | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9297c865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[30, 36, 42],\n",
       "        [66, 81, 96]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.arange(1, 7).reshape(2, 3)\n",
    "t2 = torch.arange(1, 10).reshape(3, 3)\n",
    "torch.mm(t1,t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab88d077",
   "metadata": {},
   "source": [
    "![5](https://i.loli.net/2021/01/14/gshVBOWM4QD2TiL.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c58197",
   "metadata": {},
   "source": [
    "- mv：矩阵和向量相乘      \n",
    "&emsp;&emsp;PyTorch中提供了一类非常特殊的矩阵和向量相乘的函数，矩阵和向量相乘的过程我们可以看成是先将向量转化为列向量然后再相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "278717ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "met = torch.arange(1, 7).reshape(2, 3)\n",
    "vec = torch.arange(1, 4)\n",
    "torch.mv(met, vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524946be",
   "metadata": {},
   "source": [
    "## 四、矩阵的线性代数运算\n",
    "\n",
    "|**函数**|**描述**|\n",
    "| :------:| :------: |\n",
    "| torch.trace(A)       | 矩阵的迹 |\n",
    "| matrix_rank(A)       | 矩阵的秩 |\n",
    "| torch.det(A)         | 计算矩阵A的行列式 |  \n",
    "| torch.inverse(A)        | 矩阵求逆 | \n",
    "| torch.lstsq(A,B)        | 最小二乘法 | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6189e33d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;矩阵的迹的运算相对简单，就是矩阵对角线元素之和，在PyTorch中，可以使用trace函数进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e6eafe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2], [4, 5]]).float()  \n",
    "torch.trace(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5324737",
   "metadata": {},
   "source": [
    "&emsp;&emsp;矩阵的秩（rank），是指矩阵中行或列的极大线性无关数，且矩阵中行、列极大无关数总是相同的，任何矩阵的秩都是唯一值，满秩指的是方阵（行数和列数相同的矩阵）中行数、列数和秩相同，满秩矩阵有线性唯一解等重要特性，而其他矩阵也能通过求解秩来降维，同时，秩也是奇异值分解等运算中涉及到的重要概念。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd54ba5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(1, 5).reshape(2, 2).float()\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a93d8ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\ts_env\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: torch.matrix_rank is deprecated in favor of torch.linalg.matrix_rankand will be removed in a future PyTorch release. The parameter 'symmetric' was renamed in torch.linalg.matrix_rank to 'hermitian'. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\LinearAlgebra.cpp:618.)\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matrix_rank(A) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5f3e69",
   "metadata": {},
   "source": [
    "### 3.矩阵的行列式(det)\n",
    " 所谓行列式，我们可以简单将其理解为矩阵的一个基本性质或者属性，通过行列式的计算，我们能够知道矩阵是否可逆，从而可以进一步求解矩阵所对应的线性方程。当然，更加专业的解释，行列式的作为一个基本数学工具，实际上就是矩阵进行线性变换的伸缩因子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40750dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-3.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2], [4, 5]]).float()     # 秩的计算要求浮点型张量\n",
    "torch.det(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a20f5",
   "metadata": {},
   "source": [
    "### 3.线性方程组的矩阵表达形式\n",
    "\n",
    "&emsp;&emsp;在正式进入到更进一步矩阵运算的讨论之前，我们需要对矩阵建立一个更加形象化的理解。通常来说，我们会把高维空间中的一个个数看成是向量，而由这些向量组成的数组看成是一个矩阵。例如：（1，2），（3，4）是二维空间中的两个点，矩阵A就代表这两个点所组成的矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5138ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a65238f308>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = torch.arange(1, 11).reshape(2, -1).float()\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(A[:,0], A[:, 1], 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dc650f",
   "metadata": {},
   "source": [
    "如果更进一步，我们希望在二维空间中找到一条直线，来拟合这两个点，也就是所谓的构建一个线性回归模型，我们可以设置线性回归方程如下：\n",
    "<center> $ y = ax + b $ </center>\n",
    "带入（1，2）和（3，4）两个点之后，我们还可以进一步将表达式改写成矩阵表示形式，改写过程如下\n",
    "<img src=\"https://i.loli.net/2021/01/14/UEPNYc9OjGn3J5b.jpg\" alt=\"8\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8145ce7",
   "metadata": {},
   "source": [
    "而用矩阵表示线性方程组，则是矩阵的另一种常用用途，接下来，我们就可以通过上述矩阵方程组来求解系数向量x。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f28f208",
   "metadata": {},
   "source": [
    "&emsp;&emsp;首先一个基本思路是，如果有个和A矩阵相关的另一个矩阵，假设为$A^{-1}$，可以使得二者相乘之后等于1，也就是$A * A^{-1} = 1$，那么在方程组左右两边同时左乘该矩阵，等式右边的计算结果$A^{-1} * B$就将是x系数向量的取值。而此处的$A^{-1}$就是所谓的A的逆矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782b88ba",
   "metadata": {},
   "source": [
    "在上述线性方程组求解场景中，我们已经初步看到了逆矩阵的用途，而一般来说，我们往往会通过伴随矩阵来进行逆矩阵的求解。由于伴随矩阵本身并无其他核心用途，且PyTorch中也未给出伴随矩阵的计算函数（目前），因此我们直接调用inverse函数来进行逆矩阵的计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46920898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0000e+00, -5.9605e-08],\n",
       "        [-1.1921e-07,  1.0000e+00]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.tensor([[1.0, 1], [3, 1]])\n",
    "torch.mm(torch.inverse(A), A) # 返回对应的逆矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "34c715e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = torch.tensor([2.0, 4])\n",
    "torch.mv(torch.inverse(A), B) #线性方程组求解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29679ca4",
   "metadata": {},
   "source": [
    "## 五、矩阵的分解\n",
    "\n",
    "&emsp;&emsp;矩阵的分解也是矩阵运算中的常规计算，矩阵分解也有很多种类，常见的例如QR分解、LU分解、特征分解、SVD分解等等等等，虽然大多数情况下，矩阵分解都是在形式上将矩阵拆分成几种特殊矩阵的乘积，但本质上，矩阵的分解是去探索矩阵更深层次的一些属性。本节将主要围绕特征分解和SVD分解展开讲解，更多矩阵分解的运算，我们将在后续课程中逐渐进行介绍。值得一提的是，此前的逆矩阵，其实也可以将其看成是一种矩阵分解的方式，分解之后的等式如下：      \n",
    "<center> $A = A * A^{-1} * A $ </center>      \n",
    "而大多数情况下，矩阵分解都是分解成形如下述形式      \n",
    "<center> $ A = VUD$ </center>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5f1bb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.6117e+01+0.j, -1.1168e+00+0.j, -1.2253e-07+0.j])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.arange(1, 10).reshape(3, 3).float()\n",
    "torch.linalg.eigvals(A)                 # 注，此处需要输入参数为True才会返回矩阵的特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa6a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.eig(A, eigenvectors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c3df565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.eig(\n",
       "eigenvalues=tensor([[ 1.4000e+01,  0.0000e+00],\n",
       "        [-1.6447e-07,  0.0000e+00],\n",
       "        [ 2.8710e-07,  0.0000e+00]]),\n",
       "eigenvectors=tensor([]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = torch.tensor([[1, 2, 3], [2, 4, 6], [3, 6, 9]]).float()\n",
    "torch.eig(C)                # 只有一个特征的有效值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d579fc9",
   "metadata": {},
   "source": [
    "### 2.奇异值分解（SVD）\n",
    "\n",
    "&emsp;&emsp;奇异值分解（SVD）来源于代数学中的矩阵分解问题，对于一个方阵来说，我们可以利用矩阵特征值和特征向量的特殊性质（矩阵点乘特征向量等于特征值数乘特征向量），通过求特征值与特征向量来达到矩阵分解的效果     \n",
    "<center> $ A = Q\\Lambda Q^{-1}$ </center>            \n",
    "这里，Q是由特征向量组成的矩阵，而Λ是特征值降序排列构成的一个对角矩阵（对角线上每个值是一个特征值，按降序排列，其他值为0），特征值的数值表示对应的特征的重要性。      \n",
    "在很多情况下，最大的一小部分特征值的和即可以约等于所有特征值的和，而通过矩阵分解的降维就是通过在Q、Λ 中删去那些比较小的特征值及其对应的特征向量，使用一小部分的特征值和特征向量来描述整个矩阵，从而达到降维的效果。      \n",
    "但是，实际问题中大多数矩阵是以奇异矩阵形式，而不是方阵的形式出现的，奇异值分解是特征值分解在奇异矩阵上的推广形式，它将一个维度为m×n的奇异矩阵A分解成三个部分 :      \n",
    "<center> $ A = U\\sum V^{T}$ </center>         \n",
    "其中U、V是两个正交矩阵，其中的每一行（每一列）分别被称为左奇异向量和右奇异向量，他们和∑中对角线上的奇异值相对应，通常情况下我们只需要保留前k个奇异向量和奇异值即可，其中U是m×k矩阵，V是n×k矩阵，∑是k×k的方阵，从而达到减少存储空间的效果，即      \n",
    "<center> $ A_{m*n} = U_{m*m}\\sum_{m*n}V^{T}_{n*n}\\approx U_{m*k}\\sum_{k*k}V^{T}_{k*n}$ </center>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed7a4932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.svd(\n",
       "U=tensor([[-0.2673, -0.8018, -0.5345],\n",
       "        [-0.5345, -0.3382,  0.7745],\n",
       "        [-0.8018,  0.4927, -0.3382]]),\n",
       "S=tensor([14.0000,  0.0000,  0.0000]),\n",
       "V=tensor([[-0.2673,  0.0000,  0.9636],\n",
       "        [-0.5345, -0.8321, -0.1482],\n",
       "        [-0.8018,  0.5547, -0.2224]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.svd(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ed07b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.0000, 3.0000],\n",
       "        [2.0000, 4.0000, 6.0000],\n",
       "        [3.0000, 6.0000, 9.0000]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 还原回原矩阵\n",
    "CU, CS, CV = torch.svd(C)\n",
    "torch.mm(torch.mm(CU, torch.diag(CS)), CV.t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9a9933c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 2.0000, 3.0000],\n",
       "        [2.0000, 4.0000, 6.0000],\n",
       "        [3.0000, 6.0000, 9.0000]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 部分信息还原\n",
    "U1 = CU[:, 0].reshape(3, 1) # U的第一列\n",
    "C1 = CS[0]                           # C的第一个值\n",
    "V1 = CV[:, 0].reshape(1, 3)           # V的第一行\n",
    "torch.mm((U1 * C1), V1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cb8eaf",
   "metadata": {},
   "source": [
    "# Lesson 5.基本优化思想与最小二乘法\n",
    "\n",
    "&emsp;&emsp;在正式开始进行神经网络建模之前，我们还需要掌握一些基本数学工具，在PyTorch中，最核心的基础数学工具就是梯度计算工具，也就是PyTorch的AutoGrad（自动微分）模块。虽然对于任何一个通用的深度学习框架，都会提供许多自动优化的算法和现成的loss function，PyTorch也不例外，但如果希望能够更深入的理解神经网络、希望对深度学习的建模不仅仅停留在调包和调参的层次，那我们就必须深入一些数学领域、掌握一些数学工具，从底层提升自己的数学能力，以期能够在日后的使用深度学习算法的过程中能够更加灵活的解决问题、取得更好的建模效果。而AutoGrad模块，就是PyTorch提供的最核心的数学工具模块，我们可以利用其编写一系列的最优化方法，当然，要使用好微分工具，就首先需要了解广泛应用于机器学习建模的优化思想。      \n",
    "&emsp;&emsp;所谓优化思想，指的是利用数学工具求解复杂问题的基本思想，同时也是近现代机器学习算法在实际建模过程中经常使用基础理论在实际建模过程中，我们往往会先给出待解决问题的数值评估指标，并在此基础之上构建方程、采用数学工具、不断优化评估指标结果，以期达到可以达到的最优结果。本节，我们将先从简单线性回归入手，探讨如何将机器学习建模问题转化为最优化问题，然后考虑使用数学方法对其进行求解。      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4745363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d2e99a",
   "metadata": {},
   "source": [
    "### 转化为优化问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dee3e70",
   "metadata": {},
   "source": [
    "&emsp;&emsp;上述问题除了可以使用矩阵方法求解以外，还可以将其转化为最优化问题，然后通过求解最优化问题的方法对其进行求解。总的来说，最优化问题的转化分为两步，其一是确定优化数值指标，其二则是确定优化目标函数。在大多数问题中，这二者是相辅相成的，确定了优化的数值指标，也就确定了优化的目标函数。\n",
    "\n",
    "&emsp;&emsp;如果我们希望通过一条直线拟合二维平面空间上分布的点，最核心的目标，毫无疑问，就是希望方程的预测值和真实值相差较小。假设真实的y值用y表示，预测值用ŷ表示，带入a、b参数，则有数值表示如下：\n",
    "\n",
    "|$ x^{(i)} $| $ y^{(i)} $ | $ ŷ^{(i)} $ |\n",
    "| :------:| :------: | :------ |\n",
    "| 1 | 2 | a+b |\n",
    "| 3 | 4 | 3a+b |\n",
    "| ŷ表示 | 对应预测值 | \n",
    "\n",
    "<center>$ŷ_1 = 1*a + b  = a + b$</center>      \n",
    "<center>$ŷ_2 = 3*a + b  = 3a + b$</center>\n",
    "\n",
    "&emsp;&emsp;而这两个预测值和真实值相差：\n",
    "\n",
    "<center>$y_1 = 2,  ŷ_1 = a + b, y_1 - ŷ_1 = 2 - a - b$</center>\n",
    "<center>$y_2 = 4,  ŷ_2 = 3a + b, y_2 - ŷ_1=2 = 4 - 3a - b$</center>\n",
    "\n",
    "&emsp;&emsp;我们希望ŷ和y尽可能接近，因此我们可以考虑计算上述误差总和，但为了避免正负相消（一部分为正、另一部分为负），在衡量上述两个点的误差总和时，我们使用平方和来进行衡量，而不是简单的求和：\n",
    "\n",
    "<center>$(y_1 - ŷ_1)^2 + (y_2 - ŷ_2)^2 $</center>     \n",
    "<center>$=  (2 - a - b)^2 + (4 - 3a - b)^2$</center>   \n",
    "\n",
    "&emsp;&emsp;上式也就是两个点的预测值和真实值的差值的平方和，也就是所谓的，误差平方和——SSE（Sum of the Squared Errors)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f6eea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "x = np.arange(-1,3,0.05)\n",
    "y = np.arange(-1,3,0.05)\n",
    "a, b = np.meshgrid(x, y)\n",
    "SSE = (2 - a - b) ** 2 + (4 - 3 * a - b) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb5f594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\ts_env\\lib\\site-packages\\numpy\\core\\_asarray.py:171: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.plot_surface(a, b, SSE, cmap='rainbow')\n",
    "ax.contour(a, b, SSE, zdir='z', offset=0, cmap=\"rainbow\")  #生成z方向投影，投到x-y平面\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41ebd11",
   "metadata": {},
   "source": [
    "&emsp;&emsp;初步探索函数图像，不难看出，目标函数是个整体看起来“向下凸”的函数，函数的凹凸性是函数的重要性质，首先我们给出凸函数的一般定义，对于任意一个函数，如果函数f(x)上存在任意两个点，$x_1, x_2$，且      \n",
    "<center> $ (f(x_1) + f(x_2))/2 >= f((x_1 + x_2)/2) $ </center>     \n",
    "我们就判定，这个函数是凸函数。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b20775a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-10,10,0.1)\n",
    "y = x ** 2\n",
    "plt.plot(x, y, '-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c8bdd0",
   "metadata": {},
   "source": [
    "不难看出，函数上任意两个点的y取值的均值都不小于这两个点均值的y的值。\n",
    "\n",
    "<img src=\"https://i.loli.net/2021/01/21/AfR1sjbMUVP5c6r.jpg\" alt=\"9\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174ef13a",
   "metadata": {},
   "source": [
    "而对于SSE来说，此处虽然不做证明，但对于简单线性回归的损失函数，SSE是凸函数，因此，对于$ SSE(a,b) =  (2 - a - b)^2 + (4 - 3a - b)^2 $而言，最小值点就是a、b两个参数求偏导等于0的点\n",
    "\n",
    "<center>$SSE = (y_1 - ŷ_1)^2 + (y_2 - ŷ_2)^2 $</center>     \n",
    "<center>$=  (2 - a - b)^2 + (4 - 3a - b)^2$</center>\n",
    "\n",
    "<center>$ \\frac{\\partial{SSE_(a,b)}}{\\partial{(a)}} = 0 $</center>           \n",
    "<center>$ \\frac{\\partial{SSE_(a,b)}}{\\partial{(b)}} = 0 $</center>\n",
    "\n",
    "<center>$ \\begin{align} \\frac{\\partial{SSE_{(a,b)}}}{\\partial{(a)}}  \n",
    "& = 2(2-a-b)*(-1) + 2(4-3a-b)*(-3)\\\\\n",
    "& = 20a+8b-28 \\\\\n",
    "& = 0  \\end{align} $</center>\n",
    "\n",
    "<center>$ \\begin{align} \\frac{\\partial{SSE_{(a,b)}}}{\\partial{(b)}}  \n",
    "& = 2(2-a-b)*(-1) + 2(4-3a-b)*(-1)\\\\\n",
    "& = 8a+4b-12 \\\\ \n",
    "& = 0 \\end{align} $</center>\n",
    "\n",
    "<center>$ (1)式 - (2)式*2 可得： 4a-4 = 0，a=1$ <\\center>\n",
    "<center>$ 将a=1带入(2)式 可得： 4b-4 = 0，b=1$ <\\center>\n",
    "    \n",
    "   <center> $ y = x + 1 $ </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96a100d",
   "metadata": {},
   "source": [
    "### 机器学习建模一般流程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7af9947",
   "metadata": {},
   "source": [
    "&emsp;&emsp;至此，我们就完成了一个基本的简单线性回归建模过程。当然，对于线性回归来说，有很多种建模方式，哪怕是主流的统计学和机器学习，在利用线性回归进行建模时都有不一样的流程（后续介绍机器学习基础时还会详细介绍）。此处我们是通过一个简单的例子，来介绍机器学习、包括深度学习的利用优化方法建模的一般思想，我们可以将其总结如下：\n",
    "\n",
    "- Step 1：提出基本模型      \n",
    "&emsp;&emsp;如本节中，我们试图利用一条直线（y=ax+b）去拟合二维平面空间中的点，这里我们所使用的这条直线，就是我们提出的基本模型。而在后续的深度学习的学习过程中，我们还将看到更为强大、同时也更加通用的神经网络模型。当然，不同的模型能够适用不同的场景，在提出模型时，我们往往会预设一些影响模型结构或者实际判别性能的参数，如简单线性回归中的a和b；\n",
    "\n",
    "- Step 2：确定损失函数和目标函数\n",
    "\n",
    "&emsp;&emsp;接下来，围绕建模的目标，我们需要合理设置损失函数，并在此基础之上设置目标函数，当然，在很多情况下，这二者是相同的。例如，在上述简单线性回归中，我们的建模目标就是希望y=ax+b这条直线能够尽可能的拟合(1,2)、(3,4)这两个点，或者说尽可能“穿过”这两个点，因此我们设置了SSE作为损失函数，也就是预测值和真实值的差值平方和。当然，在计算过程中不难发现，SSE是一个包含了a和b这两个变量的方程，因此SSE本身也是一个函数（a和b的二元函数），并且在线性回归中，SSE既是损失函数（用于衡量真实值和预测值差值的函数），同时也是我们的目标函数（接下来需要优化、或者说要求最小值的函数）。这里尤其需要注意的是，损失函数不是模型，而是模型参数所组成的一个函数。\n",
    "\n",
    "- Step 3：根据目标函数特性，选择优化方法，求解目标函数\n",
    "\n",
    "&emsp;&emsp;之前提到，目标函数既承载了我们优化的目标（让预测值和真实值尽可能接近），同时也是包含了模型参数的函数，因此完成建模需要确定参数、优化结果需要预测值尽可能接近真实值这两方面需求就统一到了求解目标函数最小值的过程中了，也就是说，当我们围绕目标函数求解最小值时，也就完成了模型参数的求解。当然，这个过程本质上就是一个数学的最优化过程，求解目标函数最小值本质上也就是一个最优化问题，而要解决这个问题，我们就需要灵活适用一些最优化方法。当然，在具体的最优化方法的选择上，函数本身的性质是重要影响因素，也就是说，不同类型、不同性质的函数会影响优化方法的选择。在简单线性回归中，由于目标函数是凸函数，我们根据凸函数性质，判断偏导函数取值为0的点就是最小值点，进而完成a、b的计算（也就是最小二乘法），其实就是通过函数本身的性质进行最优化方法的选取。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941bf0e3",
   "metadata": {},
   "source": [
    "## 第一个优化算法：最小二乘法\n",
    "\n",
    "&emsp;&emsp;前面提到，利用优化方法求解目标函数其实是机器学习建模过程中最为核心的环节，因此，我们有必要将围绕上述简单线性回归问题，进一步讨论最小二乘法背后的数学逻辑和优化思想，同时简单探讨数据的矩阵表示方法和基本矩阵运算。虽然最小二乘法并不是主流的深度学习损失函数的优化算法，但从最小二乘法入手了解优化算法背后的数学逻辑，却是非常有必要，同时，线性方程也是构建神经网络模型的基础，因此，我们有必要深入探讨线性模型建模细节以及最基本的优化算法：最小二乘法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37701d23",
   "metadata": {},
   "source": [
    "### 1.最小二乘法的代数表示方法\n",
    "\n",
    "&emsp;&emsp;从更加严格的数学角度出发，最小二乘法有两种表示形式，分别是代数法表示和矩阵表示。我们先看最小二乘法的代数表示方法。\n",
    "\n",
    "首先，假设多元线性方程有如下形式\n",
    "<center> $ f(x) = w_1x_1+w_2x_2+...+w_dx_d+b $ </center>\n",
    "\n",
    "令$w = (w_1,w_2,...w_d)$，$x = (x_1,x_2,...x_d)$，则上式可写为\n",
    "\n",
    "<center> $ f(x) = w^Tx+b $ </center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd991344",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5e6f4200b59e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'A' is not defined"
     ]
    }
   ],
   "source": [
    "X = A\n",
    "y   = B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800bee99",
   "metadata": {},
   "source": [
    "# Lesson 6.动态计算图与梯度下降入门"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9fe5b78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81f4d718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(1.,requires_grad = True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cdcc826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建函数关系\n",
    "y = x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d43954df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48699555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PowBackward0 at 0x230baa0e108>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bacf2024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = y + 1\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c253204",
   "metadata": {},
   "source": [
    "&emsp;&emsp;借助回溯机制，我们就能将张量的复杂计算过程抽象为一张图（Graph），例如此前我们定义的x、y、z三个张量，三者的计算关系就可以由下图进行表示。\n",
    "\n",
    "<img src=\"https://i.loli.net/2021/01/23/cgrnBZqPS3vWM6U.jpg\" alt=\"10\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8e0285",
   "metadata": {},
   "source": [
    "### 反向传播的基本过程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c0629f",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在《Lesson 5.》中，我们曾使用autograd.grad进行函数某一点的导数值得计算，其实，除了使用函数以外，我们还有另一种方法，也能进行导数运算：反向传播。当然，此时导数运算结果我们也可以有另一种解读：计算梯度结果。\n",
    "\n",
    "> 注：此处我们暂时不区分微分运算结果、导数值、梯度值三者区别，目前位置三个概念相同，后续讲解梯度下降时再进行区分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e19159ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 执行反向传播\n",
    "z.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0af8aef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor(1.,requires_grad = True)\n",
    "y = x ** 2\n",
    "z = y + 1\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f1202d",
   "metadata": {},
   "source": [
    "无论何时，我们只能计算叶节点的导数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24e987a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-6723ca8ff214>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 每次只计算一次，两次会报错\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\anaconda\\envs\\ts_env\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\anaconda\\envs\\ts_env\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m def grad(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "y.backward() # 每次只计算一次，两次会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5a628f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\ts_env\\lib\\site-packages\\torch\\_tensor.py:1104: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:475.)\n",
      "  return self._grad\n"
     ]
    }
   ],
   "source": [
    "y.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c87d38",
   "metadata": {},
   "source": [
    "### 反向传播运算注意事项"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6621b95d",
   "metadata": {},
   "source": [
    "&emsp;&emsp;尽管中间节点也可进行反向传播，但很多时候由于存在复合函数关系，中间节点反向传播的计算结果和输出节点反向传播输出结果并不相同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef22a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 若想保存中间节点的梯度，我们可以使用retain_grad()方法\n",
    "x = torch.tensor(1.,requires_grad = True)\n",
    "y = x ** 2\n",
    "y.retain_grad()\n",
    "z = y ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec69e40b",
   "metadata": {},
   "source": [
    "### 阻止计算图追踪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e06d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1.,requires_grad = True)\n",
    "y = x ** 2\n",
    "with torch.no_grad():\n",
    "    z = y ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa655294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.requires_grad # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3c7a524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 在某些情况下，我们也可以创建一个不可导的相同张量参与后续运算，从而阻断计算图的追踪\n",
    "\n",
    "x = torch.tensor(1.,requires_grad = True)\n",
    "y = x ** 2\n",
    "y1 = y.detach()\n",
    "z = y1 ** 2\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32d3c7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26321f07",
   "metadata": {},
   "source": [
    "### 识别叶节点"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ef3be0",
   "metadata": {},
   "source": [
    "&emsp;&emsp;由于叶节点较为特殊，如果需要识别在一个计算图中某张量是否是叶节点，可以使用is_leaf属性查看对应张量是否是叶节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "812ce69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b96130cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.is_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc5b15",
   "metadata": {},
   "source": [
    "### 梯度下降基本思想\n",
    "&emsp;&emsp;有了AutoGrad模块中各函数方法的支持，接下来，我们就能尝试手动构建另一个优化算法：梯度下降算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae321923",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在《Lesson 5.》中，我们尝试使用最小二乘法求解简单线性回归的目标函数，并顺利的求得了全域最优解。但正如上节所说，在所有的优化算法中最小二乘法虽然高效并且结果精确，但也有不完美的地方，核心就在于最小二乘法的使用条件较为苛刻，要求特征张量的交叉乘积结果必须是满秩矩阵，才能进行求解。而在实际情况中，很多数据的特征张量并不能满足条件，此时就无法使用最小二乘法进行求解。\n",
    "最小二乘法结果：\n",
    "$$\\hat w ^T = (X^TX)^{-1}X^Ty$$\n",
    "\n",
    "&emsp;&emsp;当最小二乘法失效的情况时，其实往往也就代表原目标函数没有最优解或最优解不唯一。针对这样的情况，有很多中解决方案，例如，我们可以在原矩阵方程中加入一个扰动项$\\lambda I$，修改后表达式如下：\n",
    "$$\\hat w ^{T*} = (X^TX + \\lambda I)^{-1}X^Ty$$\n",
    "其中，$\\lambda$是扰动项系数，$I$是单元矩阵。由矩阵性质可知，加入单位矩阵后，$(X^TX + \\lambda I)$部分一定可逆，而后即可直接求解$\\hat w^{T*}$，这也就是岭回归的一般做法。\n",
    "\n",
    "&emsp;&emsp;当然，上式修改后求得的结果就不再是全域最小值，而是一个接近最小值的点。鉴于许多目标函数本身也并不存在最小值或者唯一最小值，在优化的过程中略有偏差也是可以接受的。当然，伴随着深度学习的逐渐深入，我们会发现，最小值并不唯一存在才是目标函数的常态。基于此情况，很多根据等式形变得到的精确的求解析解的优化方法（如最小二乘）就无法适用，此时我们需要寻找一种更加通用的，能够高效、快速逼近目标函数优化目标的最优化方法。在机器学习领域，最通用的求解目标函数的最优化方法就是著名的梯度下降算法。\n",
    "\n",
    "&emsp;&emsp;值得一提的是，我们通常指的梯度下降算法，并不是某一个算法，而是某一类依照梯度下降基本理论基础展开的算法簇，包括梯度下降算法、随机梯度下降算法、小批量梯度下降算法等等。接下来，我们就从最简单的梯度下降入手，讲解梯度下降的核心思想和一般使用方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120c6572",
   "metadata": {},
   "source": [
    "### 2.梯度下降核心思想\n",
    "\n",
    "&emsp;&emsp;梯度下降的基本思想其实并不复杂，其核心就是希望能够通过数学意义上的迭代运算，从一个随机点出发，一步步逼近最优解。\n",
    "例如，在此前求解简单线性回归方程的过程中，我们曾查看SSE的三维函数图像如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22741976",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\ts_env\\lib\\site-packages\\numpy\\core\\_asarray.py:171: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "x = np.arange(-1,3,0.05)\n",
    "y = np.arange(-1,3,0.05)\n",
    "a, b = np.meshgrid(x, y)\n",
    "SSE = (2 - a - b) ** 2 + (4 - 3 * a - b) ** 2\n",
    "\n",
    "fig = plt.figure()  \n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.plot_surface(a, b, SSE, cmap='rainbow')\n",
    "ax.contour(a, b, SSE, zdir='z', offset=0, cmap=\"rainbow\")  #生成z方向投影，投到x-y平面\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab8ad22",
   "metadata": {},
   "source": [
    "而梯度下降，作为最优化算法，核心目标也是找到或者逼近最小值点，而其基本过程则：\n",
    "- 在目标函数上随机找到一个初始点；\n",
    "- 通过迭代运算，一步步逼近最小值点；\n",
    "\n",
    "<img src=\"https://i.loli.net/2021/01/23/caNAtMFI5dqhfH6.jpg\" alt=\"11\" style=\"zoom:50%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3269cb3f",
   "metadata": {},
   "source": [
    "### 梯度下降的方向与步长\n",
    "\n",
    "&emsp;&emsp;当然，梯度下降的基本思想好理解，但实现起来却并不容易（这也是大多数机器学习算法的常态）。在实际沿着目标函数下降的过程中，我们核心需要解决两个问题，其一是往哪个方向走，其二是每一步走多远。以上述简单线性回归的目标函数为例，在三维空间中，目标函数上的每个点理论上都有无数个移动的方向，每次移动多远的物理距离也没有明显的约束，而这些就是梯度下降算法核心需要解决的问题，也就是所谓的方向和步长。\n",
    "\n",
    "首先，是关于方向的讨论。\n",
    "\n",
    "> 关于方向的讨论，其实梯度下降是采用了一种局部最优推导全域最优的思路，我们首先是希望能够找到让目标函数变化最快的方向作为移动的方向，而这个方向，就是梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e7e6b5",
   "metadata": {},
   "source": [
    "####  导数与梯度\n",
    "\n",
    "&emsp;&emsp;我们都知道，函数上某一点的导数值的几何含义就是函数在该点上切线的斜率。例如y=x**2中，x在1点的导数就是函数在1点的切线的斜率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee529fac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-901e1de8e6da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m                          \u001b[1;31m# y = 2x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m                       \u001b[1;31m# 在（1，1）点的切线方程\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r-'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "x = np.arange(-10,10,0.1)\n",
    "y = x ** 2                          # y = 2x\n",
    "z = 2 * x - 1                       # 在（1，1）点的切线方程\n",
    "plt.plot(x, y, '-')\n",
    "plt.plot(x, z, 'r-')\n",
    "plt.plot(1, 1, 'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc36c88",
   "metadata": {},
   "source": [
    "更进一步来讲，对于上述函数，x取值为1的时候，导数和切线的斜率为2，代表含义是给1这个点一个无穷小的增量，1只能沿着切向方向移动（但仍然在曲线上）。当然，该点导数值的另外一个解释就是该点的梯度，梯度的值（grad）和导数相同，而梯度的概念可以视为导数概念的延申，只不过梯度更侧重方向的概念，也就是从梯度的角度解读导数值，就代表着当前这个点的可以使得y值增加最快的移动方向。\n",
    "\n",
    "梯度：梯度本身是一个代表方向的矢量，代表某一函数在该点处沿着梯度方向变化时，变化率最大。当然，梯度的正方向代表函数值增长最快的方向，梯度的负方向表示函数减少最快的方向。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d62d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1., requires_grad = True)\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5ed7a4",
   "metadata": {},
   "source": [
    "不过此时由于自变量存在一维空间，只能沿着x轴变化（左右移动，只有两个方向），梯度给出的方向只能解读为朝着2，也就是正方向变化时，y的增加最快（确实如此，同时也显而易见）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb0d52",
   "metadata": {},
   "source": [
    "#### 梯度与方向\n",
    "\n",
    "&emsp;&emsp;为了更好的解读梯度与方向之间的关系，我们以《Lesson 5.》中简单线性回归损失函数为例来进行查看。我们有目标函数及其图像如下：\n",
    "\n",
    "<center>$SSE_{(a, b)} =  (2 - a - b)^2 + (4 - 3a - b)^2$</center>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aae51db8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-2ebf156e76f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'3d'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_surface\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSSE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'rainbow'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontour\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSSE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'z'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"rainbow\"\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#生成z方向投影，投到x-y平面\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig = plt.figure()  \n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.plot_surface(a, b, SSE, cmap='rainbow')\n",
    "ax.contour(a, b, SSE, zdir='z', offset=0, cmap=\"rainbow\")  #生成z方向投影，投到x-y平面\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(0., requires_grad = True)\n",
    "b = torch.tensor(0., requires_grad = True)\n",
    "s0 = torch.pow((2 - a - b), 2) + torch.pow((4 - 3 * a - b), 2)\n",
    "s0.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7af1f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8559d0c",
   "metadata": {},
   "source": [
    "也就是原点和（-28，-12）这个点之间连成直线的方向，就是能够使得sse变化最快的方向，并且朝向（-28，-12）方向就是使得sse增加最快的方向，反方向则是令sse减少最快的方向。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c609e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过绘制直线，确定原点的移动方向\n",
    "x = np.arange(-30,30,0.1)\n",
    "y = (12/28) * x                     \n",
    "plt.plot(x, y, '-')\n",
    "plt.plot(0, 0, 'ro')\n",
    "plt.plot(-28, -12, 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6a054d",
   "metadata": {},
   "source": [
    "**Point:**这里有关于方向的两点讨论      \n",
    "- 方向没有大小，虽然这是个显而易见的观点，但我们当我们说朝着（-28，-12）方向移动，只是说沿着直线移动，并非一步移动到（-28，-12）上；      \n",
    "- 方向跟随梯度，随时在发生变化。值得注意的是，一旦点发生移动，梯度就会随之发生变化，也就是说，哪怕是沿着让sse变化最快的方向移动，一旦“沿着方向”移动了一小步，这个方向就不再是最优方向了。\n",
    "\n",
    "当然，逆梯度值的方向变化是使得sse变小的最快方向，我们尝试移动“一小步”。一步移动到(28,12)是没有意义的，梯度各分量数值的绝对值本身也没有距离这个层面的数学含义。由于a和b的取值要按照（28，12）等比例变化，因此我们不妨采用如下方法进行移动：\n",
    "\n",
    "$$ \\left [\\begin{array}{cccc}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\end{array}\\right] + 0.01 * \n",
    "\\left [\\begin{array}{cccc}\n",
    "28 \\\\\n",
    "12 \\\\\n",
    "\\end{array}\\right] =\n",
    "\\left [\\begin{array}{cccc}\n",
    "0.28 \\\\\n",
    "0.12 \\\\\n",
    "\\end{array}\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a648fd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(0.28, requires_grad = True)\n",
    "b = torch.tensor(0.12, requires_grad = True)\n",
    "s1 = (2 - a - b) ** 2 + (4 - 3 * a - b) ** 2\n",
    "s1.backward()\n",
    "a.grad, b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5409fef",
   "metadata": {},
   "source": [
    "## 四、梯度下降的数学表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966091b6",
   "metadata": {},
   "source": [
    "### 1.梯度下降的代数表示\n",
    "\n",
    "&emsp;&emsp;根据上述描述过程，我们可以通过代数运算方式总结梯度下降运算的一般过程\n",
    "\n",
    "令多元线性回归方程为\n",
    "<center> $ f(x) = w_1x_1+w_2x_2+...+w_dx_d+b $ </center>\n",
    "令\n",
    "$$\\hat w = (w_1,w_2,...,w_d,b)$$\n",
    "$$\\hat x = (x_1,x_2,...,x_d,1)$$\n",
    "出于加快迭代收敛速度的目标，我们在定义梯度下降的损失函数L时，在原SSE基础上进行比例修正，新的损失函数$L(w_1,w_2,...,w_d,b) = \\frac{1}{2m}SSE$，其中，m为样本个数。\n",
    "\n",
    "损失函数有：\n",
    " $$\n",
    "   L(w_1,w_2,...,w_d,b) = \\frac{1}{2m}\\sum_{j=0}^{m}(f(x_1^{(j)}, x_2^{(j)}, ...1) - y_j)^2\n",
    "   $$\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080dc476",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.zeros(2, 1, requires_grad = True)\n",
    "# 特征张量\n",
    "X = torch.tensor([[1.,1],[3, 1]], requires_grad = True)\n",
    "y = torch.tensor([2.,4], requires_grad = True).reshape(2,1)\n",
    "# 设置步长\n",
    "eps = torch.tensor(0.01, requires_grad = True)\n",
    "# 梯度计算公式\n",
    "grad = torch.mm(X.t(), (torch.mm(X, weights) - y))/2\n",
    "weights = weights - eps * grad\n",
    "for k in range(3):\n",
    "    grad = torch.mm(X.t(), (torch.mm(X, weights) - y))/2\n",
    "    weights = weights - eps * grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cd7819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradDescent(X, y, eps = torch.tensor(0.01, requires_grad = True), numIt = 1000):\n",
    "    m, n = X.shape\n",
    "    weights = torch.zeros(n, 1, requires_grad = True)\n",
    "    for k in range(numIt):\n",
    "        grad = torch.mm(X.t(), (torch.mm(X, weights) - y))/2\n",
    "        weights = weights - eps * grad\n",
    "    return weights\n",
    "\n",
    "X = torch.tensor([[1.,1],[3, 1]], requires_grad = True)\n",
    "y = torch.tensor([2.,4], requires_grad = True).reshape(2,1)\n",
    "gradDescent(X, y)\n",
    "weights = gradDescent(X, y, numIt = 10000)\n",
    "torch.mm((torch.mm(X,weights)-y).t(), torch.mm(X,weights)-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39662371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b4290b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts_env",
   "language": "python",
   "name": "ts_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
